<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>DoE Supercomputer Gets 400,000 AI Cores - PicoVibe</title><script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content="One of the more interesting AI silicon projects over the last couple of years has been the Cerebras Wafer Scale Engine, most notably for the fact that a single chip is the size of a literal wafer. Cerebras packs the WSE1 chip into a 15U custom liquid cooled server, called the CS-1, with a number"><meta name=robots content="index,follow,noarchive"><meta property="og:title" content="DoE Supercomputer Gets 400,000 AI Cores"><meta property="og:description" content="One of the more interesting AI silicon projects over the last couple of years has been the Cerebras Wafer Scale Engine, most notably for the fact that a single chip is the size of a literal wafer. Cerebras packs the WSE1 chip into a 15U custom liquid cooled server, called the CS-1, with a number"><meta property="og:type" content="article"><meta property="og:url" content="/cerebras-wafer-scale-engine-news-doe-supercomputer-gets-400000-ai-cores.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-07-03T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-03T00:00:00+00:00"><meta itemprop=name content="DoE Supercomputer Gets 400,000 AI Cores"><meta itemprop=description content="One of the more interesting AI silicon projects over the last couple of years has been the Cerebras Wafer Scale Engine, most notably for the fact that a single chip is the size of a literal wafer. Cerebras packs the WSE1 chip into a 15U custom liquid cooled server, called the CS-1, with a number"><meta itemprop=datePublished content="2024-07-03T00:00:00+00:00"><meta itemprop=dateModified content="2024-07-03T00:00:00+00:00"><meta itemprop=wordCount content="876"><meta itemprop=keywords content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/mainroad/css/style.css><link rel="shortcut icon" href=./favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=./index.html title=PicoVibe rel=home><div class="logo__item logo__text"><div class=logo__title>PicoVibe</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>DoE Supercomputer Gets 400,000 AI Cores</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2024-07-03T00:00:00Z>July 03, 2024</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=./categories/blog/ rel=category>blog</a></span></div></div></header><div class="content post__content clearfix"><p>One of the more interesting AI silicon projects over the last couple of years has been the Cerebras Wafer Scale Engine, most notably for the fact that a single chip is the size of a literal wafer. Cerebras packs the WSE1 chip into a 15U custom liquid cooled server, called the CS-1, with a number of innovations regarding packaging, power, and setup. A single CS-1 requires about 20 kW of power peak, and costs around a couple million dollars (the Pittsburgh Supercomputing Center purchased two last year based on a $5m research grant). Cerebras say they have double digit customers and several dozen units already in the field, however today marks a considerable milestone as the US Department of Energy now has one deployed and working, attached directly to a supercomputer.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16022/cs1_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>It is fairly easy to be marveled at Cerebras’ design, which involves a number of innovations and patents regarding cross-reticle connectivity, and the 400,000 AI cores in the processor are designed to withstand manufacturing defects by offering spare cores throughout the design that can be used in place. This gives each wafer a very good yield – every chip off the production line is useable. Cerebras is a solutions provider, not just a chip designer, and as a result it sells WSE1 in a self-contained 15U rack-mountable unit. The aim of the CS-1 design is that it can fit into any standard datacenter, you plug in the power and the networking, and away you go.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16022/2019-11-19%2014.03.04_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>On the software side of things, Cerebras has its own graph compiler for its chip, which accepts AI networks based on TensorFlow and pyTorch, and then configures them in the most optimal way around the massive chip. This year at Hot Chips, the company went into some detail about how programming at the wafer scale works, with the compiler balancing compute and on-chip resources to use the best area-vs-performance tradeoffs for any given AI function or kernel. Each kernel can be split in model parallel or data parallel fashion in order to generate the optimal communication pattern, maximizing the performance and ultimately the on-chip use. Users can also hand-optimize kernels if required. The graph compiler is also designed to make efficient work of sparse networks, especially with having such a large chip and 18 GB of onboard SRAM as well as a full MIMD layout.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16022/202008182314251_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The news today however is that Cerebras is announcing a very important CS-1 installation. It may only be a single unit, but the Lawrence Livermore National Laboratory (LLNL), as funded by the US Department of Energy’s National Nuclear Security Administration, has purchased a CS-1 unit and bolted it onto its 23 PetaFLOP ‘Lassen’ Supercomputer. The Lassen supercomputer is a 684-node Power9 + Volta + Infiniband supercomputer, rated at around 23 PetaFLOPs and currently <a href=#>sits at #14 on the TOP500 list</a>.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16022/AI_875x500_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The goal of adding a CS-1 to Lassen is to help accelerate and offload targeted AI research and modelling assistance. Lassen is primarily used for nuclear simulations, material science, drug design, and medical analysis. These projects often involve longer-than-the-universe search spaces of computational analysis, and these projects are starting to use AI training and inference to help reduce the search spaces and optimize where the compute is needed, reducing wasted compute and minimizing errors. This will be one of the goals of the CS-1 attached to Lassen, both for training these search space models and then applying them at a larger scale through inference. This will be the first time that LLNL has added AI-specific hardware to its computational repertoire, and the term being used for attaching AI acceleration onto HPC compute is being called Cognitive Simulation, or CogSim for short.</p><p>“We need new answers for how to improve our ability to meet our mission requirements and to respond to ever-increasing computational requirements. Cognitive simulation is an approach that we believe could drive continued exponential capability improvements, and a system-level heterogeneous approach based on novel architectures such as the Cerebras CS-1 are an important part of achieving those improvements.” said Bronis R. de Supinski, Chief Technology Officer for Livermore Computing, who led the CS-1 procurement effort.</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16022/202008182321081_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>In discussing with Cerebras, we got an insight into how buying one of these systems works. From initial discussions to deployment to actively being used by the researchers took less than 60 days. Enabling the CS-1 through a workload manager was little more than a single line of Slurm, and users are allocated compute time on the CS-1 through a time-division multiplex queue. Currently the CS-1 cannot support multiple users simultaneously, which Cerebras told us was one of the trade-offs in getting the WSE1 to market in the time scale they have done. Future iterations are likely to work towards this goal.</p><p>As part of the sale, Cerebras and LLNL are engaging in a new AI Center of Excellence (AICoE) in order to develop and build the optimal parameters to accelerate this type of cognitive simulation into the lab workflow. Depending on the results, according to the press release, this may lead to additional Cerebras systems being attached to Lassen in the future. These could be CS-1, or potentially the new WSE2 that Cerebras <a href=#>teased at the end of its Hot Chips 2020 talk</a>.</p><p>Source: <a href=#>LLNL</a>, Cerebras</p><h3>Related Reading</h3><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH53fJFrZpydopqvs63SZq6anpWnerSvwKWcZp2enLavsYynnLCrXZm8pnnSrqeeqpOkurHB056pZp%2BVqcBugI9pZ2loXZa2bq%2FOq5ys</p></div></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=./msi-unveils-cubi-5-10m-palmsized-pc-comet-lake-64-gb-of-ram-wifi-6.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>Comet Lake with 64 GB of RAM &amp;amp; Wi-Fi 6</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=./did-kirk-herbstreit-ever-play-nfl.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Did Kirk Herbstreit ever play in the NFL?</p></a></div></nav></div><aside class=sidebar><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=./bastian-schweinsteiger.html>Bastian Schweinsteiger- Age, Girlfriend, Net Worth, Weight, Height, Ethnicity</a></li><li class=widget__item><a class=widget__link href=./alex-romer-height-weight-net-worth-age-birthday-wikipedia-who-nationality-biography-675086-html.html>Alex Romer Height, Weight, Net Worth, Age, Birthday, Wikipedia, Who, Nationality, Biography</a></li><li class=widget__item><a class=widget__link href=./are-seasons-1-5-of-billions-on-netflix-4846-html.html>Are Seasons 1-5 of Billions on Netflix?</a></li><li class=widget__item><a class=widget__link href=./jasper-p-c3-a4-c3-a4kk-c3-b6nen.html>Jasper Pkknen movie reviews &amp;amp; film summaries</a></li><li class=widget__item><a class=widget__link href=./casper-smart-net-worth-133804.html>Casper Smart Net Worth</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=./categories/blog/>blog</a></li></ul></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2024 PicoVibe.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=https://assets.cdnweb.info/hugo/mainroad/js/menu.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>